package org.rti.ttfinder.wrapper;
import android.app.Activity;
import android.content.Context;
import android.graphics.Bitmap;
import android.os.SystemClock;
import android.os.Trace;
import android.util.Log;

import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.support.common.FileUtil;
import org.tensorflow.lite.support.common.TensorProcessor;
import org.tensorflow.lite.support.label.TensorLabel;
import org.tensorflow.lite.DataType;
import org.tensorflow.lite.support.image.TensorImage;
import org.tensorflow.lite.support.tensorbuffer.TensorBuffer;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.PriorityQueue;

import static java.lang.Math.min;

public class TTClassifierWithFeaturesInterpreter  extends TTInterpreter{

    private ByteBuffer allFeaturesBuffer;
    //private MappedByteBuffer preProcessor;

    TTProcessSegmentation segmentProcessor;

    /** Labels corresponding to the output of the vision model. */
    private List<String> labels;
    private static final int MAX_RESULTS = 1; //This will probably change in future implementations
    private List<TTClassifierWithFeaturesInterpreter.Recognition> topKResults = null;

    @Override
    protected String getModelPath() {
        return "full.tflite";
    }

    /** An immutable result returned by a Classifier describing what was recognized. */
    public static class Recognition {
        /** Display name for the recognition. */
        private final String title;

        /** Likeliness of the recognition */
        private final String likeliness;

        /**
         * A sortable score for how good the recognition is relative to others. Higher should be better.
         */
        private final Float confidence; //this is really, a weight generated by classifier


        public Recognition(
                final String title, final Float confidence) {
            this.title = title;
            this.confidence = confidence;
            if(confidence >= 0.9) {
                likeliness = "Certain";
            }
            else if(confidence  >= 0.75) {
                likeliness = "Almost Certain";
            }
            else if(confidence >= 0.55) {
                likeliness = "Likely";
            }
            else
            {
                likeliness = "Unlikely";
            }
        }

        public String getTitle() {
            return title;
        }

        public Float getConfidence() {
            return confidence;
        }

        @Override
        public String toString() {
            String resultString = "";

            if (title != null) {
                resultString += title;
            }

            if (confidence != null) {
                resultString += " :: " + String.format("%.2f", confidence);
                //resultString += " :: " + likeliness; //removing likeliness text for now
            }

            return resultString.trim();
        }
    }

    @Override
    public boolean runInference(final Bitmap segmentationBitmap) {
        Log.v(TAG, "Inference from bitmap not implemented anymore");
        return true;
    }

    public boolean runInference(final TTProcessSegmentation segmentProcessor){
        lastInferenceDetails.setLength(0);
        lastInferenceTimeCost = 0;
        topKResults = null;

        // Get the subimages in the input segmentation image
        // Make sure segment processor has the right number of images:
        if (segmentProcessor == null || segmentProcessor.getNumSubImages()!=NUM_SUBIMAGES){
            Log.e(TAG, "Segment processor does not have the right number of subimages");
            return false;
        }

        // Create a feature buffer using the output of the bitmaps in the segment processor.
        Trace.beginSection("createClassifyInput");
        long startTimeForInputBufferBuild = SystemClock.uptimeMillis();
        int inputTensorIndex = 0;
        DataType imageDataType = tflite.getInputTensor(inputTensorIndex).dataType();
        if(allFeaturesBuffer == null) {
            allFeaturesBuffer = ByteBuffer.allocate(3*NUM_SUBIMAGES * SUB_IMAGE_SIZE * SUB_IMAGE_SIZE * imageDataType.byteSize());
        }
        else
            allFeaturesBuffer.rewind();
        updateLastInferenceRunDetails("Capacity of output buffer :: " + allFeaturesBuffer.capacity());
        updateLastInferenceRunDetails("Subimage edge length :: " + SUB_IMAGE_SIZE + "px");

        for(int subImageInd=0; subImageInd < NUM_SUBIMAGES; subImageInd++){
            Bitmap subImage = segmentProcessor.getNthSubImage(subImageInd);
            if(subImage == null){
                Log.e(TAG, "Subimage at " + subImageInd + " is null");
            }
            TensorImage tmp = new TensorImage(imageDataType);
            tmp.load(subImage);
            allFeaturesBuffer.put(tmp.getBuffer());
        }
        Log.v(TAG, "Size of all features buffer: " + allFeaturesBuffer.capacity() + " " + allFeaturesBuffer.position());
//                Ideally will use the code below to preprocess the whole stack
//        try (Interpreter preprocessor = new Interpreter(preProcessor)) {
//            int outputTensorIndex = 0;
//            int[] outputShape =
//                    preprocessor.getOutputTensor(outputTensorIndex).shape();
//            Log.v(TAG, "Output Shape from Preprocessor is supposed to be: " + Arrays.toString(outputShape));
//            DataType outputDataType = preprocessor.getOutputTensor(outputTensorIndex).dataType();
//            ByteBuffer ob = ByteBuffer.allocate(3*NUM_SUBIMAGES * SUB_IMAGE_SIZE * SUB_IMAGE_SIZE * imageDataType.byteSize());
//            preprocessor.run(allFeaturesBuffer.rewind(), ob.rewind());
//            allFeaturesBuffer.rewind();
//            allFeaturesBuffer.put((ByteBuffer) ob.rewind());
//        }
//        Log.v(TAG, "Size of all features buffer: " + allFeaturesBuffer.capacity() + " " + allFeaturesBuffer.position());

//        int size = SUB_IMAGE_SIZE*SUB_IMAGE_SIZE;
//        int[] intValues = new int[size];
//        float[] floatValues = new float[size*3];
//        for(int subimageInd=0; subimageInd<NUM_SUBIMAGES; subimageInd++) {
//            Bitmap subImage = segmentProcessor.getNthSubImage(subimageInd);
//            if(subImage == null){
//                Log.e(TAG, "Subimage at " + subimageInd + " is null");
//            }
//
//            long startTimeForPreprocessSubImage = SystemClock.uptimeMillis();
//            subImage.getPixels(intValues, 0, subImage.getWidth(), 0, 0, subImage.getWidth(), subImage.getHeight());
//            // The preprocessing steps below are very specific to Resnet50 caffe flavor.
//            // Explanation is here: https://github.com/keras-team/keras/blob/3a33d53ea4aca312c5ad650b4883d9bac608a32e/keras/applications/imagenet_utils.py#L197
//            // Preprocessing for Resnet50 on desktop looks different from the phone. (https://stackoverflow.com/questions/56685995/resnet50-image-preprocessing)
//            // The implementation is suggested in: https://stackoverflow.com/questions/56034981/how-to-fix-the-image-preprocessing-difference-between-tensorflow-and-android-stu
//            for (int i = 0; i < intValues.length; ++i) {
//                // this is a ARGB format, so we need to mask the least significant 8 bits to get blue, and next 8 bits to get green and next 8 bits to get red. Since we have an opaque image, alpha can be ignored.
//                final int val = intValues[i];
//
//                // what I think it should be to do the same thing in mode caffe when using keras
//                floatValues[i * 3 + 0] = (((val >> 16) & 0xFF) - (float)123.68);
//                floatValues[i * 3 + 1] = (((val >> 8) & 0xFF) - (float)116.779);
//                floatValues[i * 3 + 2] = (((val & 0xFF)) - (float)103.939);
//            }
//            TensorImage tmp = new TensorImage(imageDataType);
//            // get the proper shape array
//            tmp.load(subImage);
//            int[] shape = tmp.getTensorBuffer().getShape();
//            // load the float values generated above.
//            tmp.load(floatValues, shape);
//            long endTimeForPreprocessSubImage = SystemClock.uptimeMillis();
//            logSectionTimeCost("subImagePreprocessing_" + subimageInd, (endTimeForPreprocessSubImage - startTimeForPreprocessSubImage));
//            ByteBuffer tempBB = tmp.getBuffer();
//            tempBB.rewind();
//            allFeaturesBuffer.put(tempBB);
//        }



        Trace.endSection();
        long endTimeForInputBufferBuild = SystemClock.uptimeMillis();
        logSectionTimeCost("createClassifyInput",  (endTimeForInputBufferBuild - startTimeForInputBufferBuild));

        int[] imageShape = tflite.getInputTensor(0).shape(); // {1, height, width, 3}
        Log.v(TAG, "Classifier Input Shape is supposed to be: " + Arrays.toString(imageShape));
        int[] outputShape =
                tflite.getOutputTensor(0).shape(); // {1, NUM_CLASSES}
        Log.v(TAG, "Classifier Output Shape is supposed to be: " + Arrays.toString(outputShape));

        // Logs this method so that it can be analyzed with systrace.
        Trace.beginSection("ClassifyInputFeatures");
        // Runs the inference call.
        Trace.beginSection("runInference");
        long startTimeForReference = SystemClock.uptimeMillis();
        tflite.run(allFeaturesBuffer, outputBuffer.getBuffer().rewind());
        long endTimeForReference = SystemClock.uptimeMillis();
        Trace.endSection();
        logSectionTimeCost("runInference", (endTimeForReference - startTimeForReference));

        Trace.beginSection("runPostProcessing");
        long startTimeForPostProcessing = SystemClock.uptimeMillis();

        Log.v(TAG, "Output buffer shape is: " + outputBuffer.getShape()[0] + " " + outputBuffer.getShape());
        float activation = outputBuffer.getFloatArray()[0];
        int result_label = Math.round(activation);
        Log.v(TAG, "Activation is:  " +activation);
        Log.v(TAG, "Resulting label is:  " +result_label);
        Map<String, Float> labeledProbability = new HashMap<>();
        if(result_label == 0) {
            activation = 1.0f - activation; //Just a hack to make sure that the
        }
        labeledProbability.put(labels.get(result_label), activation);
        labeledProbability.put(labels.get(1-result_label), 1.f - activation);
//        TensorProcessor probabilityProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();
//        Map<String, Float> labeledProbability =
//                new TensorLabel(labels, probabilityProcessor.process(outputBuffer))
//                        .getMapWithFloatValue();
        long endTimeForPostProcessing = SystemClock.uptimeMillis();
        Trace.endSection();
        logSectionTimeCost("runPostProcessing", (endTimeForPostProcessing - startTimeForPostProcessing));

        String msg = "";
        for (Map.Entry<String, Float> entry : labeledProbability.entrySet()) {
            msg =  msg + "Weights for label " + entry.getKey() + " :: " + entry.getValue() + "\n";
        }
        updateLastInferenceRunDetails(msg);
        topKResults = getTopKProbability(labeledProbability);
        Trace.endSection();
        updateLastInferenceRunDetails("Time cost for classification  :: " + lastInferenceTimeCost + "ms");

        return true;
    }

    /** Gets the top-k results. */
    private static List<TTClassifierWithFeaturesInterpreter.Recognition> getTopKProbability(Map<String, Float> labelProb) {
        // Find the best classifications.
        PriorityQueue<TTClassifierWithFeaturesInterpreter.Recognition> pq =
                new PriorityQueue<>(
                        MAX_RESULTS,
                        new Comparator<TTClassifierWithFeaturesInterpreter.Recognition>() {
                            @Override
                            public int compare(TTClassifierWithFeaturesInterpreter.Recognition lhs, TTClassifierWithFeaturesInterpreter.Recognition rhs) {
                                // Intentionally reversed to put high confidence at the head of the queue.
                                return Float.compare(rhs.getConfidence(), lhs.getConfidence());
                            }
                        });

        for (Map.Entry<String, Float> entry : labelProb.entrySet()) {
            pq.add(new TTClassifierWithFeaturesInterpreter.Recognition(entry.getKey(), entry.getValue()));
        }

        final ArrayList<TTClassifierWithFeaturesInterpreter.Recognition> recognitions = new ArrayList<>();
        int recognitionsSize = min(pq.size(), MAX_RESULTS);
        for (int i = 0; i < recognitionsSize; ++i) {
            recognitions.add(pq.poll());
        }
        return recognitions;
    }

    public List<TTClassifierWithFeaturesInterpreter.Recognition> getTopKResults() {
        return topKResults;
    }

    /**
     * Initializes a {@code TTSegmentInterpreter}.
     *
     * @param activity
     */
    public TTClassifierWithFeaturesInterpreter(Context activity, Device device, int numThreads)
            throws IOException {
        super(activity, device, numThreads, false);
        labels = new ArrayList<>();
        labels.add("Healthy");
        labels.add("TT");
        allFeaturesBuffer = null;
        TAG ="TTClassifierWithFeaturesInterpreter";
        //@TODO: change the path to tflite path with proper inputs
        //preProcessor = FileUtil.loadMappedFile(activity, "preprocess.tflite");
    }
}
